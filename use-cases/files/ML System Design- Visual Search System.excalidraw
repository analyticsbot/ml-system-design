{
  "type": "excalidraw",
  "version": 2,
  "source": "https://excalidraw.com",
  "elements": [
    {
      "id": "Xe1e7dr6m_lpDuj40_0xa",
      "type": "text",
      "x": 421.2265625,
      "y": 224.75390625,
      "width": 1265.259391784668,
      "height": 3500,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b5H",
      "roundness": null,
      "seed": 111034790,
      "version": 3773,
      "versionNonce": 293177530,
      "isDeleted": false,
      "boundElements": null,
      "updated": 1736197997921,
      "link": null,
      "locked": false,
      "text": "ML System Design: Visual Search System\n\nClarification/ Assumptions\nFrame as ML problem\nData\n    - Data prep\n    - Feature Engineering\n    - Creating train/val/test + imbalance\nML Models\nEvaluation\nModel deployment and Monitoring\n\nClarification\n\n    - visual search system such as Pinterest, or Google reverse image search\n        - which website in mind\n    - what data we have available to images?\n    - does it need to be personalize to the user?\n    - is there an existing system is place?\n        - what is the expectation from this new system? what are the success criteria for the model?\n        - is it better accuracy or latency\n\n    - do we have any labeled data? such as pair of images which are similar and dissimilar\n        - can we trust the labeled data?\n\nFrame as ML problem\n    - retrieve images that are similar to the given image in real-time. Users won't wait a long time\n\n    I/O\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    2 step process\n        - step 1: retrieve candidates that are similar to the given image\n        - step 2: personalize the results to the user/ ranking of images\n\n\nData\n    \n    - Data prep\n\n        - user\n            - user id\n            - any demographic information available: location\n            - language\n            - device\n            - avg duration active over past X days\n            - types of images category/subcategory interacted in last x days\n            - CTR in last X days\n\n        - image\n            - image\n            - tags\n            - likes/shares (if available)\n            - number of impressions in last X days\n            - number of total impressions since added\n            - when it was added\n            - user who added\n\n        - user image interaction\n            - user - image - interaction type (click, impression, etc) - timestamp\n\n    - Feature Engineering\n        - scale/ normalize: impressions/ likes/ shares\n        - bucketize + one hot encoding: time of the day\n        - embedding: city, device\n        - embedding using a pre-trained model: image, tags\n\n    - Creating train/test/val split\n        - step 1:\n            image pairs that are similar, images of dogs, cats etc\n            image pairs that are not similar, such as dog and cat, also need to have hard negatives\n\n        - step 2:\n            X: features we talked earlier\n            y: 1 or 0 (clicked or not clicked)\n            imbalanced data (many few clicks as compared to not clicks)\n\nML Model\n\n    - no training possible\n        step 1:\n            - use a off the shelf model such as CLIP, use it to extract embeddings for all images and store in a vectordb\n            - for the query image, extract the embedding and search in the vector db\n        \n        step 2:\n            - rank based on a combination or recency, likes, shares, host (business metrics)\n\n    - training possible\n        step 1:\n            - train the embeddings using the image dataset created above. Representation learning\n            - use the positive and negative pairs with contrastive loss\n        \n        step 2:\n            - logistic regression: sort by the output confidence score\n                pros: easy to train, faster inference\n                cons: wont work good with non-linear data, multicollinearity\n\n            - GBDT: \n                pros: robust to bias/variance issue, distributed training available\n                cons: too many hyperparameters, not suitable for continual learning\n\n            - Learning to Rank:\n                - pointwise\n                - pairwise\n                - listwise\n\n                pros: suitable for this task, incorporates ranking as the output\n                cons: needs labeled data for training (especially pairwise and listwise)\n\n    Recommendation: start with GBDT and then move to LTR\n\nEvaluation\n    - loss: \n        step 1: contrastive loss\n        step 2: CE\n\n    - offline:\n        precision@k, recall@k, MRR, NDCG\n\n    - online:\n        CTR, user report rate\n\n        \n        \n\n        \n",
      "fontSize": 20,
      "fontFamily": 5,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "ML System Design: Visual Search System\n\nClarification/ Assumptions\nFrame as ML problem\nData\n    - Data prep\n    - Feature Engineering\n    - Creating train/val/test + imbalance\nML Models\nEvaluation\nModel deployment and Monitoring\n\nClarification\n\n    - visual search system such as Pinterest, or Google reverse image search\n        - which website in mind\n    - what data we have available to images?\n    - does it need to be personalize to the user?\n    - is there an existing system is place?\n        - what is the expectation from this new system? what are the success criteria for the model?\n        - is it better accuracy or latency\n\n    - do we have any labeled data? such as pair of images which are similar and dissimilar\n        - can we trust the labeled data?\n\nFrame as ML problem\n    - retrieve images that are similar to the given image in real-time. Users won't wait a long time\n\n    I/O\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    2 step process\n        - step 1: retrieve candidates that are similar to the given image\n        - step 2: personalize the results to the user/ ranking of images\n\n\nData\n    \n    - Data prep\n\n        - user\n            - user id\n            - any demographic information available: location\n            - language\n            - device\n            - avg duration active over past X days\n            - types of images category/subcategory interacted in last x days\n            - CTR in last X days\n\n        - image\n            - image\n            - tags\n            - likes/shares (if available)\n            - number of impressions in last X days\n            - number of total impressions since added\n            - when it was added\n            - user who added\n\n        - user image interaction\n            - user - image - interaction type (click, impression, etc) - timestamp\n\n    - Feature Engineering\n        - scale/ normalize: impressions/ likes/ shares\n        - bucketize + one hot encoding: time of the day\n        - embedding: city, device\n        - embedding using a pre-trained model: image, tags\n\n    - Creating train/test/val split\n        - step 1:\n            image pairs that are similar, images of dogs, cats etc\n            image pairs that are not similar, such as dog and cat, also need to have hard negatives\n\n        - step 2:\n            X: features we talked earlier\n            y: 1 or 0 (clicked or not clicked)\n            imbalanced data (many few clicks as compared to not clicks)\n\nML Model\n\n    - no training possible\n        step 1:\n            - use a off the shelf model such as CLIP, use it to extract embeddings for all images and store in a vectordb\n            - for the query image, extract the embedding and search in the vector db\n        \n        step 2:\n            - rank based on a combination or recency, likes, shares, host (business metrics)\n\n    - training possible\n        step 1:\n            - train the embeddings using the image dataset created above. Representation learning\n            - use the positive and negative pairs with contrastive loss\n        \n        step 2:\n            - logistic regression: sort by the output confidence score\n                pros: easy to train, faster inference\n                cons: wont work good with non-linear data, multicollinearity\n\n            - GBDT: \n                pros: robust to bias/variance issue, distributed training available\n                cons: too many hyperparameters, not suitable for continual learning\n\n            - Learning to Rank:\n                - pointwise\n                - pairwise\n                - listwise\n\n                pros: suitable for this task, incorporates ranking as the output\n                cons: needs labeled data for training (especially pairwise and listwise)\n\n    Recommendation: start with GBDT and then move to LTR\n\nEvaluation\n    - loss: \n        step 1: contrastive loss\n        step 2: CE\n\n    - offline:\n        precision@k, recall@k, MRR, NDCG\n\n    - online:\n        CTR, user report rate\n\n        \n        \n\n        \n",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "Ys0RsAP3PaI-5icGBib13",
      "type": "rectangle",
      "x": 518.7578125,
      "y": 910.85546875,
      "width": 173.2421875,
      "height": 69.234375,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b5J",
      "roundness": {
        "type": 3
      },
      "seed": 1078713638,
      "version": 73,
      "versionNonce": 1990258170,
      "isDeleted": false,
      "boundElements": [
        {
          "type": "text",
          "id": "PzeJO-4WawqN6D0x1IgGO"
        },
        {
          "id": "HKqxMsNruKsW7KHN3n5en",
          "type": "arrow"
        }
      ],
      "updated": 1736171281183,
      "link": null,
      "locked": false
    },
    {
      "id": "PzeJO-4WawqN6D0x1IgGO",
      "type": "text",
      "x": 544.2989480793476,
      "y": 920.47265625,
      "width": 122.15991634130478,
      "height": 50,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b5K",
      "roundness": null,
      "seed": 609788922,
      "version": 23,
      "versionNonce": 875236666,
      "isDeleted": false,
      "boundElements": null,
      "updated": 1736171276612,
      "link": null,
      "locked": false,
      "text": "user uploads\nimage",
      "fontSize": 20,
      "fontFamily": 5,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "Ys0RsAP3PaI-5icGBib13",
      "originalText": "user uploads image",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "HKqxMsNruKsW7KHN3n5en",
      "type": "arrow",
      "x": 695.7265625,
      "y": 945.953125,
      "width": 137.35546875,
      "height": 1.171875,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b5L",
      "roundness": {
        "type": 2
      },
      "seed": 2077925030,
      "version": 61,
      "versionNonce": 1331012922,
      "isDeleted": false,
      "boundElements": null,
      "updated": 1736171281183,
      "link": null,
      "locked": false,
      "points": [
        [
          0,
          0
        ],
        [
          137.35546875,
          -1.171875
        ]
      ],
      "lastCommittedPoint": null,
      "startBinding": {
        "elementId": "Ys0RsAP3PaI-5icGBib13",
        "focus": 0.035390878688820175,
        "gap": 3.7265625,
        "fixedPoint": null
      },
      "endBinding": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "elbowed": false
    },
    {
      "id": "Psq6IYJjVzhsmLadDX4X7",
      "type": "rectangle",
      "x": 839.71484375,
      "y": 882.60546875,
      "width": 232.06640625,
      "height": 124.453125,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b5M",
      "roundness": {
        "type": 3
      },
      "seed": 1718403558,
      "version": 71,
      "versionNonce": 905562490,
      "isDeleted": false,
      "boundElements": [
        {
          "type": "text",
          "id": "099M_yZkHNVNNdXUcPPA1"
        },
        {
          "id": "eISeyyKaIwSF9k8dQV7F7",
          "type": "arrow"
        },
        {
          "id": "Kvhb3KPtrZng9_SrU0PFP",
          "type": "arrow"
        }
      ],
      "updated": 1736171302030,
      "link": null,
      "locked": false
    },
    {
      "id": "099M_yZkHNVNNdXUcPPA1",
      "type": "text",
      "x": 856.0581283569336,
      "y": 932.33203125,
      "width": 199.3798370361328,
      "height": 25,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b5N",
      "roundness": null,
      "seed": 1585350374,
      "version": 22,
      "versionNonce": 1151145574,
      "isDeleted": false,
      "boundElements": null,
      "updated": 1736171288736,
      "link": null,
      "locked": false,
      "text": "visual search system",
      "fontSize": 20,
      "fontFamily": 5,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "Psq6IYJjVzhsmLadDX4X7",
      "originalText": "visual search system",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "eISeyyKaIwSF9k8dQV7F7",
      "type": "arrow",
      "x": 1077.3359375,
      "y": 944.74609375,
      "width": 111.5703125,
      "height": 0.6953125,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b5O",
      "roundness": {
        "type": 2
      },
      "seed": 861862842,
      "version": 35,
      "versionNonce": 1901567014,
      "isDeleted": false,
      "boundElements": null,
      "updated": 1736171291893,
      "link": null,
      "locked": false,
      "points": [
        [
          0,
          0
        ],
        [
          111.5703125,
          0.6953125
        ]
      ],
      "lastCommittedPoint": null,
      "startBinding": {
        "elementId": "Psq6IYJjVzhsmLadDX4X7",
        "focus": -0.013402451078583653,
        "gap": 5.5546875,
        "fixedPoint": null
      },
      "endBinding": null,
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "elbowed": false
    },
    {
      "id": "wCVy18v_uhnhRseRCf4D_",
      "type": "rectangle",
      "x": 848.109375,
      "y": 1084.26171875,
      "width": 229.1328125,
      "height": 100.35546875,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b5P",
      "roundness": {
        "type": 3
      },
      "seed": 735877818,
      "version": 73,
      "versionNonce": 1997668346,
      "isDeleted": false,
      "boundElements": [
        {
          "id": "Kvhb3KPtrZng9_SrU0PFP",
          "type": "arrow"
        }
      ],
      "updated": 1736171302030,
      "link": null,
      "locked": false
    },
    {
      "id": "LO2SVslWaqHiK0LO5XUq5",
      "type": "text",
      "x": 929,
      "y": 1122,
      "width": 140.3399292230606,
      "height": 25,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b5Q",
      "roundness": null,
      "seed": 1302224698,
      "version": 16,
      "versionNonce": 418318970,
      "isDeleted": false,
      "boundElements": null,
      "updated": 1736171298354,
      "link": null,
      "locked": false,
      "text": "pool of images",
      "fontSize": 20,
      "fontFamily": 5,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "pool of images",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "Kvhb3KPtrZng9_SrU0PFP",
      "type": "arrow",
      "x": 949.53515625,
      "y": 1075.91796875,
      "width": 0.3125,
      "height": 71.8359375,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b5R",
      "roundness": {
        "type": 2
      },
      "seed": 2062761318,
      "version": 32,
      "versionNonce": 58978490,
      "isDeleted": false,
      "boundElements": null,
      "updated": 1736171302030,
      "link": null,
      "locked": false,
      "points": [
        [
          0,
          0
        ],
        [
          0.3125,
          -71.8359375
        ]
      ],
      "lastCommittedPoint": null,
      "startBinding": {
        "elementId": "wCVy18v_uhnhRseRCf4D_",
        "focus": -0.11669853112009622,
        "gap": 8.34375,
        "fixedPoint": null
      },
      "endBinding": {
        "elementId": "Psq6IYJjVzhsmLadDX4X7",
        "focus": 0.048516359498784155,
        "gap": 1,
        "fixedPoint": null
      },
      "startArrowhead": null,
      "endArrowhead": "arrow",
      "elbowed": false
    },
    {
      "id": "f9GXVMggT2BjcL_gHt1J2",
      "type": "rectangle",
      "x": 1196.58984375,
      "y": 821.58984375,
      "width": 111.9453125,
      "height": 260.296875,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b5S",
      "roundness": {
        "type": 3
      },
      "seed": 1316605542,
      "version": 32,
      "versionNonce": 1960657510,
      "isDeleted": false,
      "boundElements": null,
      "updated": 1736171304792,
      "link": null,
      "locked": false
    },
    {
      "id": "VA2tBPALXqzsAH23Zskav",
      "type": "text",
      "x": 1217.12890625,
      "y": 882.39453125,
      "width": 73.49995422363281,
      "height": 150,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b5T",
      "roundness": null,
      "seed": 384741882,
      "version": 74,
      "versionNonce": 1393498150,
      "isDeleted": false,
      "boundElements": null,
      "updated": 1736171322246,
      "link": null,
      "locked": false,
      "text": "image 1\nimage 2\n\n\n\nimage N",
      "fontSize": 20,
      "fontFamily": 5,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "image 1\nimage 2\n\n\n\nimage N",
      "autoResize": true,
      "lineHeight": 1.25
    }
  ],
  "appState": {
    "gridSize": 20,
    "gridStep": 5,
    "gridModeEnabled": false,
    "viewBackgroundColor": "#ffffff"
  },
  "files": {}
}